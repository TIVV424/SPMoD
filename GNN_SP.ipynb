{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoine/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import networkx as nx\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import dgl\n",
    "import dgl.data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from dgl.data import DGLDataset\n",
    "import urllib\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from dgl.nn import GraphConv\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import timedelta\n",
    "import network_build\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = \"2022-06-01\"\n",
    "network_metrics = pd.read_csv(\"./Database/network_metrics_%s.csv\" % date, index_col=0)\n",
    "SP = pd.read_csv(\"./Database/SP_60_%s.csv\" % date, index_col=0)\n",
    "\n",
    "# Use of chunk size because a problem of memory\n",
    "\n",
    "chunk_size = 10000 \n",
    "chunks_iterator = pd.read_csv(\"./Database/order_clean_260.csv\",index_col=0, chunksize=chunk_size)\n",
    "chunks=[]\n",
    "for chunk in chunks_iterator:\n",
    "    chunks.append(chunk)\n",
    "order = pd.concat(chunks)\n",
    "order[\"call_time\"] = pd.to_datetime(order[\"call_time\"])\n",
    "order[\"end_time\"] = pd.to_datetime(order[\"end_time\"])\n",
    "start_time = pd.to_datetime(date + \" 06:00:00 AM\")\n",
    "end_time = pd.to_datetime(date + \" 12:00:00 PM\")\n",
    "order_one_day = order[(order[\"call_time\"] >= start_time) & (order[\"call_time\"] < end_time)]\n",
    "\n",
    "SP.columns = [\"SP\"]\n",
    "SP = SP.reset_index(drop=True)\n",
    "order_one_day = order_one_day.sort_values(by=\"call_time\").reset_index(drop=True)\n",
    "network_metrics = network_metrics.reset_index(drop=True)\n",
    "\n",
    "# the length of the three dataframes should be the same\n",
    "assert len(order_one_day) == len(network_metrics) == len(SP)\n",
    "\n",
    "# the index of the three dataframes should be the same\n",
    "assert order_one_day.index.equals(network_metrics.index) \n",
    "assert SP.index.equals(network_metrics.index)\n",
    "\n",
    "\n",
    "df = pd.concat([order_one_day, network_metrics, SP], axis=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "t_opt = timedelta(days=0,hours = 0, minutes=20); t_lock = timedelta(days=0,hours = 0, minutes=10) ; t0  = pd.to_datetime(\"2022-06-01 06:00:00 AM\")\n",
    "t= t0\n",
    "i=0\n",
    "area = np.load(\"./Database/NY_area.npy\")\n",
    "nodes = pd.DataFrame(columns=['nodes','graph_id'])\n",
    "edges = pd.DataFrame(columns=['src','dst','graph_id'])\n",
    "progress_bar = tqdm(total=30, desc=\"Progression\")\n",
    "while t+t_opt <= pd.to_datetime(\"2022-06-01 11:00:00 AM\"):\n",
    "\n",
    "    # the start time and end time are [t, t+t_opt)]\n",
    "    order_start_time = t\n",
    "    order_end_time = t+t_opt\n",
    "\n",
    "    # this is very important to make sure >= in call time and < in end time\n",
    "    order_pick = order_one_day[(order_one_day[\"call_time\"] >= order_start_time) & (order_one_day[\"call_time\"] < order_end_time)]\n",
    "    order_pick = order_pick[[\"sid\", \"call_time\", \"eid\", \"end_time\"]]\n",
    "\n",
    "    net = network_build.ConstructNetwork(order_pick.values, area, void = 10)\n",
    "    G_order = net.build_network(network_type=\"order\")\n",
    "    G_order.add_node('sink')\n",
    "    for node in G_order.nodes():\n",
    "        if node != 'sink':\n",
    "            G_order.add_edge(node, 'sink', weight=0)\n",
    "    current_nodes = pd.DataFrame(G_order.nodes,columns=[\"nodes\"])\n",
    "    current_nodes[\"graph_id\"] = np.full(len(G_order.nodes),i,dtype=int)\n",
    "\n",
    "    current_edges = pd.DataFrame(G_order.edges,columns = ['src','dst'])\n",
    "    current_edges[\"graph_id\"] = np.full(len(G_order.edges),i,dtype=int)\n",
    "\n",
    "    nodes = pd.concat([nodes,current_nodes],ignore_index=True)\n",
    "    edges = pd.concat([edges,current_edges],ignore_index=True)\n",
    "    progress_bar.update(1)\n",
    "    i+=1\n",
    "    t+=t_lock\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.to_csv('nodes_t_opt_20_t_lock_10.csv')\n",
    "edges.to_csv('edges_t_opt_20_t_lock_10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Data(DGLDataset):\n",
    "    def __init__(self, t_opt,t_lock,date):\n",
    "        self.t_opt = t_opt\n",
    "        self.t_lock = t_lock\n",
    "        self.date = date\n",
    "        super().__init__(name=\"synthetic\")\n",
    "        \n",
    "    def process(self):\n",
    "\n",
    "        self.t0 = pd.to_datetime(self.date + \" 06:00:00 AM\")\n",
    "        t = self.t0\n",
    "\n",
    "        network_metrics = pd.read_csv(\"./Database/network_metrics_%s.csv\" % self.date, index_col=0)\n",
    "        SP = pd.read_csv(\"./Database/SP_60_%s.csv\" % self.date, index_col=0)\n",
    "\n",
    "        # Use of chunk size because of a problem of memory\n",
    "\n",
    "        chunk_size = 10000 \n",
    "        chunks_iterator = pd.read_csv(\"./Database/order_clean_260.csv\",index_col=0, chunksize=chunk_size)\n",
    "        chunks=[]\n",
    "        for chunk in chunks_iterator:\n",
    "            chunks.append(chunk)\n",
    "        order = pd.concat(chunks)\n",
    "        order[\"call_time\"] = pd.to_datetime(order[\"call_time\"])\n",
    "        order[\"end_time\"] = pd.to_datetime(order[\"end_time\"])\n",
    "        start_time = pd.to_datetime(self.date + \" 06:00:00 AM\")\n",
    "        end_time = pd.to_datetime(self.date + \" 12:00:00 PM\")\n",
    "        order_one_day = order[(order[\"call_time\"] >= start_time) & (order[\"call_time\"] < end_time)]\n",
    "\n",
    "        SP.columns = [\"SP\"]\n",
    "        SP = SP.reset_index(drop=True)\n",
    "        order_one_day = order_one_day.sort_values(by=\"call_time\").reset_index(drop=True)\n",
    "        network_metrics = network_metrics.reset_index(drop=True)\n",
    "\n",
    "        # the length of the three dataframes should be the same\n",
    "        assert len(order_one_day) == len(network_metrics) == len(SP)\n",
    "\n",
    "        # the index of the three dataframes should be the same\n",
    "        assert order_one_day.index.equals(network_metrics.index) \n",
    "        assert SP.index.equals(network_metrics.index)\n",
    "\n",
    "\n",
    "        df = pd.concat([order_one_day, network_metrics, SP], axis=1).dropna()\n",
    "\n",
    "        \n",
    "        nodes = pd.read_csv('./nodes_t_opt_%s_t_lock_%s.csv' % (str(int(self.t_opt.seconds/60)),str(int(self.t_lock.seconds/60))),index_col = 0)\n",
    "        edges = pd.read_csv('./edges_t_opt_%s_t_lock_%s.csv' % (str(int(self.t_opt.seconds/60)),str(int(self.t_lock.seconds/60))), index_col = 0)\n",
    "\n",
    "        self.graphs = []\n",
    "        i=0\n",
    "        while t + self.t_opt <= pd.to_datetime(\"%s 11:00:00 AM\" % self.date):\n",
    "            cur_nodes = nodes[nodes[\"graph_id\"]==i]\n",
    "            num_nodes = len(cur_nodes)\n",
    "            cur_edges = edges[edges[\"graph_id\"]==i]\n",
    "            # i rename every nodes as 'i' instead of 'ti' and replace sink by '-1'\n",
    "            cur_edges = cur_edges.replace(\"sink\",\"t-1\")   \n",
    "            cur_edges[\"src\"]= cur_edges[\"src\"].str.slice(1).astype(int)\n",
    "            cur_edges[\"dst\"]= cur_edges[\"dst\"].str.slice(1).astype(int)\n",
    "\n",
    "            src = torch.from_numpy(cur_edges[\"src\"].to_numpy())\n",
    "            dst = torch.from_numpy(cur_edges[\"dst\"].to_numpy())\n",
    "\n",
    "            nodes_feature = df[(df['call_time']>=t )& (df['call_time']<t+self.t_opt)]\n",
    "\n",
    "            sink = {'sid' : 0,\t'call_time': t+self.t_opt,\t'eid':0,\t'end_time' : t + self.t_opt,\t'delta_min' : 0,\t'degree' : 0,\t'in_degree' : 0,\t'out_degree': 0, 'betweenness_centrality' :0,\t'closeness_centrality': 0,\t'katz_centrality' : 0, 'SP' :0}\n",
    "            sink_df = pd.DataFrame([sink])\n",
    "            nodes_feature = pd.concat([nodes_feature, sink_df], ignore_index=True)\n",
    "            labels = nodes_feature.pop('SP')\n",
    "\n",
    "            # change date time into seconds between 6 am and the current time\n",
    "\n",
    "            call_list = nodes_feature[\"call_time\"]\n",
    "            call_num = [int(ts.timestamp()-(self.t0).timestamp()) for ts in call_list]\n",
    "\n",
    "            nodes_feature[ \"call_time\"] = call_num\n",
    "\n",
    "            end_list = nodes_feature[\"end_time\"]\n",
    "            end_num = [int(ts.timestamp()-(self.t0).timestamp()) for ts in end_list]\n",
    "\n",
    "            nodes_feature[\"end_time\"] = end_num\n",
    "\n",
    "            nodes_feature = torch.from_numpy(nodes_feature.to_numpy())\n",
    "            labels = torch.from_numpy(labels.to_numpy())\n",
    "\n",
    "            self.feature_dim = 11\n",
    "\n",
    "            g = dgl.graph((src, dst), num_nodes=num_nodes)\n",
    "            g.ndata[\"feat\"] = nodes_feature\n",
    "            g.ndata[\"labels\"] = labels\n",
    "            g.ndata[\"graph_id\"] = torch.from_numpy(np.zeros(nodes_feature.shape[0])+i)\n",
    "\n",
    "            self.graphs.append(g)\n",
    "            i+=1\n",
    "            t+=self.t_lock\n",
    "    def __getitem__(self, i):\n",
    "        return self.graphs[i],self.graphs[i].ndata[\"labels\"]\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't use it for the moment\n",
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance=5, min_delta=0):\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss):\n",
    "        if (validation_loss - train_loss) > self.min_delta:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.tolerance:  \n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_model(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(GCN_model, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size,allow_zero_in_degree=True)\n",
    "        self.conv2 = GraphConv(hidden_size, num_classes,allow_zero_in_degree=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = self.conv1(g, features)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        h= F.relu(h)\n",
    "        h=self.dropout(h)\n",
    "        g.ndata['h']=h\n",
    "        \n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = My_Data(t_opt=timedelta(minutes=20),t_lock=timedelta(minutes=10),date = \"2022-06-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe Kernel s’est bloqué lors de l’exécution du code dans la cellule active ou une cellule précédente. Veuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. Cliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. Pour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "# I tryed differnet batch_size \n",
    "batch_size = 1\n",
    "input_dim = 4\n",
    "hidden_dim = 32\n",
    "num_classes = 17\n",
    "lr = 0.001\n",
    "n_epoch = 10\n",
    "\n",
    "node_feature_dim = dataset.feature_dim\n",
    "\n",
    "num_examples = len(dataset)\n",
    "num_train = int(num_examples * 0.8)\n",
    "num_val = int(num_examples * 0.1)\n",
    "\n",
    "t = torch.arange(num_train)\n",
    "idx = torch.randperm(t.nelement())\n",
    "t = t.view(-1)[idx].view(t.size())\n",
    "train_sampler = SubsetRandomSampler(t)\n",
    "\n",
    "t = torch.arange(num_train, num_train+num_val)\n",
    "idx = torch.randperm(t.nelement())\n",
    "t = t.view(-1)[idx].view(t.size())\n",
    "val_sampler = SubsetRandomSampler(t)\n",
    "\n",
    "t = torch.arange(num_train+num_val, num_examples)\n",
    "idx = torch.randperm(t.nelement())\n",
    "t = t.view(-1)[idx].view(t.size())\n",
    "test_sampler = SubsetRandomSampler(t)\n",
    "\n",
    "train_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=train_sampler,batch_size=batch_size, drop_last=False, shuffle=False,\n",
    ")\n",
    "val_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=val_sampler, batch_size=batch_size, drop_last=False, shuffle=False,\n",
    ")\n",
    "test_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=test_sampler, batch_size=batch_size, drop_last=False, shuffle=False,\n",
    ")\n",
    "\n",
    "model = GCN_model(node_feature_dim, hidden_dim, num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "early_stopping = EarlyStopping(tolerance=5, min_delta=10)\n",
    "\n",
    "epoch_losses = []; val_losses = []\n",
    "for epoch in range(n_epoch):\n",
    "    epoch_loss = 0\n",
    "    for iter, (batched_graph, labels) in enumerate(train_dataloader):\n",
    "        \n",
    "        pred = model(batched_graph, batched_graph.ndata[\"feat\"].float())\n",
    "        \n",
    "        loss = F.cross_entropy(pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "    epoch_loss /= (iter + 1)\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    \n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for iter, (batched_graph, labels) in enumerate(val_dataloader):\n",
    "            pred = model(batched_graph, batched_graph.ndata[\"feat\"].float())\n",
    "            loss = F.cross_entropy(pred, labels)\n",
    "            val_loss += loss.detach().item()\n",
    "        val_loss /= (iter + 1)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    early_stopping(epoch_loss, val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"We are at epoch:\", epoch)\n",
    "        break\n",
    "    print(str(epoch)+': '+str(epoch_loss)+': '+str(val_loss))\n",
    "\n",
    "plt.title('cross entropy averaged over minibatches')\n",
    "plt.plot(epoch_losses, label='Train')\n",
    "plt.plot(val_losses, label='Test')\n",
    "plt.show()\n",
    "\n",
    "num_correct = 0\n",
    "num_tests = 0\n",
    "\n",
    "pred_batch = []; id_batch = []\n",
    "for batched_graph, labels in test_dataloader:\n",
    "    graphs = dgl.unbatch(batched_graph)\n",
    "\n",
    "    for i in range(len(graphs)):\n",
    "        id_batch += [int(graphs[i].ndata[\"graphid\"][0].numpy())]\n",
    "\n",
    "    pred = model(batched_graph, batched_graph.ndata[\"feat\"].float())\n",
    "    num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "    pred_batch += list((pred.argmax(1)).numpy())\n",
    "    \n",
    "    num_tests += len(labels)\n",
    "\n",
    "\n",
    "print(\"Test accuracy:\", num_correct / num_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8ebb5d28ea55755674f1a738a1deff2071a53c72288e971b28f2d2652420cbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
